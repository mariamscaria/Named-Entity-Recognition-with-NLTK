{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7632c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring Named Entity Recognition with NLTK: A Beginner's Guide.\n",
    "In the vast landscape of information available online, it can be challenging to extract meaningful insights from articles. That's where our project steps in, employing a clever tool called Named Entity Recognition (NER). Think of NER as a friendly guide that helps computers recognize and categorize important details like names of people, places, and organizations within a sea of words.\n",
    "\n",
    "Our project specifically focuses on reading news articles. Using the Natural Language Toolkit (NLTK), our system breaks down sentences, identifies the roles of words (like names or locations), and highlights significant information. It's like having a language wizard that points out crucial details, making it easier to understand the main points of an article. This not only aids in digesting information quickly but also helps readers, researchers, and analysts make more informed decisions.\n",
    "\n",
    "In essence, our project is all about simplifying the complexity of text. By applying NER to news articles, we're providing a tool that enhances comprehension, making it accessible for anyone navigating through a barrage of information. Whether you're a student, researcher, or someone curious about current events, our project strives to make the wealth of online knowledge more understandable and user-friendly.\n",
    "\n",
    "## Module 1\n",
    "### Task 1: Importing Necessary Libraries and Resources.\n",
    "Before understanding the process of Named Entity Recognition, we need to import the necessary libraries and view the news article. Let's take a look!\n",
    "\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "#--- Read in text file(text.txt) ----\n",
    "with open('./text.txt', 'r') as file:\n",
    "    my_text = file.read()\n",
    "\n",
    "# ---WRITE YOUR CODE FOR TASK 1 ---\n",
    "#--- Inspect data ---\n",
    "my_text[:500]\n",
    "\n",
    "### Task 2: Tokenizing the Sentence.\n",
    "\n",
    "Fantastic!!! We have checked the resource. Now, we need to split each word into a separate element. By doing this, we enable our system to analyze and understand the structure of the text more effectively. Let's get it done, buddy!\n",
    "\n",
    "# Uncommented the below code and run once. After the code is excecuted, please commented the below code\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "#--- WRITE YOUR CODE FOR TASK 2 ---\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_words = word_tokenize(my_text)\n",
    "\n",
    "#--- Inspect data ---\n",
    "tokenized_words\n",
    "\n",
    "### Task 3: Performing Part-of-Speech Tagging.\n",
    "Great!!! We have converted the text to tokens. Now, we need to perform Part-of-speech tagging, which assigns grammatical categories (like nouns or verbs) to each word. This process helps us grasp the finer details of the sentence's language and contributes to the overall comprehension of the text. Let's work on it.\n",
    "\n",
    "# Uncommented the below code and run once. After the code is excecuted, please commented the below code\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#--- WRITE YOUR CODE FOR TASK 3 ---\n",
    "from nltk import pos_tag\n",
    "pos_tags = pos_tag(tokenized_words)\n",
    "\n",
    "#--- Inspect data ---\n",
    "pos_tags\n",
    "\n",
    "### Task 4: Named Entity Recognition and Continuous Extraction.\n",
    "Wow!!! We have assigned grammatical categories to each word. Now, we need to identify and classify entities within the text. Let's get started!\n",
    "\n",
    "# Uncommented the below code and run once. After the code is excecuted, please commented the below code\n",
    "# import nltk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "#--- WRITE YOUR CODE FOR TASK 4 ---\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "prev = None\n",
    "continuous_chunk = []\n",
    "current_chunk = []\n",
    "\n",
    "for subtree in ner_tree:\n",
    "    if isinstance(subtree, Tree):  # Check if the subtree is a named entity\n",
    "        entity_type = subtree.label()  # Get the type of the entity\n",
    "        entity = \" \".join([word for word, tag in subtree.leaves()])  # Get the named entity\n",
    "        if prev != entity_type:  # Avoid duplicates\n",
    "            continuous_chunk.append((entity_type, entity))\n",
    "        prev = entity_type\n",
    "    else:\n",
    "        if prev:\n",
    "            continuous_chunk.append((prev, \" \".join([word for word, tag in current_chunk])))\n",
    "            prev = None\n",
    "        current_chunk = []\n",
    "\n",
    "\n",
    "if current_chunk:\n",
    "    continuous_chunk.append((prev, \" \".join([word for word, tag in current_chunk])))\n",
    "\n",
    "\n",
    "continuous_chunk_with_types = list(set(continuous_chunk))\n",
    "\n",
    "#--- Inspect data ---\n",
    "continuous_chunk_with_types\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
